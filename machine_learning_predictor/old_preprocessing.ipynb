{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2de99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8cca9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, image as mpimg\n",
    "import pandas as pd\n",
    "from difficulty_levels import DifficultyLevels\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "download_folder = \"tracking_data_download\"\n",
    "labeled_images_folder = \"labeled_images\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "NUMBER_OF_CLASSES = 3\n",
    "\n",
    "results_folder = \"ml_results\"\n",
    "data_folder_path = os.path.join(\"..\", \"post_processing\", download_folder)\n",
    "# print(data_folder_path)\n",
    "\n",
    "NEW_IMAGE_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef980c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0071b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_plot(train_history, epochs, metric=\"categorical_accuracy\", output_folder=results_folder,\n",
    "                     output_name=\"train_history.png\"):\n",
    "\n",
    "    acc = train_history.history[f\"{metric}\"]\n",
    "    val_acc = train_history.history[f\"val_{metric}\"]\n",
    "    loss = train_history.history[\"loss\"]\n",
    "    val_loss = train_history.history[\"val_loss\"]\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    # save plot to file and show in a new window\n",
    "    plt.savefig(os.path.join(output_folder, output_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4e4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_participant_images(participant_folder, use_folder=False, use_step_size=False):\n",
    "    post_processing_folder_path = os.path.join(\"..\", \"post_processing\")\n",
    "    participant_folder_path = os.path.join(post_processing_folder_path, download_folder, participant_folder)\n",
    "\n",
    "    # iterate over the csv file and yield the image paths and their corresponding difficulty level\n",
    "    images_label_log = os.path.join(participant_folder_path, \"labeled_images.csv\")\n",
    "    labeled_images_df = pd.read_csv(images_label_log)\n",
    "\n",
    "    # FIXME unfortunately a different order changes the results :(\n",
    "    # for difficulty_level in [\"easy\", \"hard\", \"medium\"]:\n",
    "    for difficulty_level in labeled_images_df.difficulty.unique():\n",
    "        # create a subset of the df that contains only the rows with this difficulty level\n",
    "        sub_df = labeled_images_df[labeled_images_df.difficulty == difficulty_level]\n",
    "\n",
    "        for idx, row in sub_df.iterrows():\n",
    "            image_path = row[\"image_path\"]\n",
    "            full_image_path = os.path.join(post_processing_folder_path, image_path)\n",
    "            # current_image = cv2.imread(full_image_path)\n",
    "            yield full_image_path, difficulty_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526768b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_test_data(data):\n",
    "    train_test_data = []\n",
    "\n",
    "    for participant in data:\n",
    "        for image_path, difficulty_level in get_participant_images(participant, use_folder=False, use_step_size=False):\n",
    "            label_vector = DifficultyLevels.get_one_hot_encoding(difficulty_level)\n",
    "            try:\n",
    "                # grayscale_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                # use this instead: (for reason see\n",
    "                # https://stackoverflow.com/questions/37203970/opencv-grayscale-mode-vs-gray-color-conversion#comment103382641_37208336)\n",
    "                color_img = cv2.imread(image_path)\n",
    "                grayscale_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                resized_img = cv2.resize(grayscale_img, (NEW_IMAGE_SIZE, NEW_IMAGE_SIZE))\n",
    "                train_test_data.append([np.array(resized_img, dtype=np.uint8), label_vector, image_path])\n",
    "\n",
    "            except Exception as e:\n",
    "                sys.stderr.write(f\"\\nError in reading and resizing image '{image_path}': {e}\")\n",
    "\n",
    "    # random.shuffle(train_test_data)\n",
    "    return train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf12583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_preprocessing():\n",
    "    set_random_seed(RANDOM_SEED)  # for reproducibility\n",
    "\n",
    "    without_participants = [\"participant_1\", \"participant_2\", \"participant_4\", \"participant_5\", \"participant_6\",\n",
    "                            \"participant_7\", \"participant_8\", \"participant_9\", \"participant_11\", \"participant_12\",\n",
    "                            \"participant_13\"]\n",
    "\n",
    "    all_participants = os.listdir(data_folder_path)\n",
    "    # remove some participants for testing\n",
    "    all_participants = [p for p in all_participants if p not in set(without_participants)]\n",
    "\n",
    "    random.shuffle(all_participants)\n",
    "\n",
    "    train_ratio = 0.8\n",
    "    train_split = int(len(all_participants) * train_ratio)\n",
    "    train_participants = all_participants[:train_split]\n",
    "    test_participants = all_participants[train_split:]\n",
    "    print(f\"{len(train_participants)} participants used for training: {train_participants}\")\n",
    "    print(f\"{len(test_participants)} participants used for validation: {test_participants}\")\n",
    "\n",
    "    train_data = preprocess_train_test_data(train_participants)\n",
    "    test_data = preprocess_train_test_data(test_participants)\n",
    "    print(\"Len training data: \", len(train_data))\n",
    "    print(\"Len test data: \", len(test_data))\n",
    "\n",
    "    # TODO save them all as one and split later when reading in?\n",
    "    train_images = []  # features for training\n",
    "    train_labels = []  # labels for training\n",
    "    train_paths = []  # paths to images in train data\n",
    "    for img_data, label, path in train_data:\n",
    "        train_images.append(img_data)\n",
    "        train_labels.append(label)\n",
    "        train_paths.append(path)\n",
    "\n",
    "    test_images = []  # features for testing\n",
    "    test_labels = []  # labels for testing\n",
    "    test_paths = []  # paths to images in test data\n",
    "    for img_data, label, path in test_data:\n",
    "        test_images.append(img_data)\n",
    "        test_labels.append(label)\n",
    "        test_paths.append(path)\n",
    "\n",
    "    train_images = np.asarray(train_images).reshape(-1, NEW_IMAGE_SIZE, NEW_IMAGE_SIZE, 1)\n",
    "    test_images = np.asarray(test_images).reshape(-1, NEW_IMAGE_SIZE, NEW_IMAGE_SIZE, 1)\n",
    "    # normalize all images to [0, 1] for the neural network\n",
    "    train_images = train_images.astype('float32') / 255.0\n",
    "    test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "    result_folder = \"ml_results\"\n",
    "    if not os.path.exists(result_folder):\n",
    "        os.mkdir(result_folder)\n",
    "\n",
    "    # TODO use np.savez() to save compressed?\n",
    "    np.save(os.path.join(result_folder, \"train_images.npy\"), train_images, allow_pickle=False)\n",
    "    np.save(os.path.join(result_folder, \"train_labels.npy\"), train_labels, allow_pickle=False)\n",
    "    np.save(os.path.join(result_folder, \"train_paths.npy\"), train_paths, allow_pickle=False)\n",
    "\n",
    "    np.save(os.path.join(result_folder, \"test_images.npy\"), test_images, allow_pickle=False)\n",
    "    np.save(os.path.join(result_folder, \"test_labels.npy\"), test_labels, allow_pickle=False)\n",
    "    np.save(os.path.join(result_folder, \"test_paths.npy\"), test_paths, allow_pickle=False)\n",
    "    \n",
    "    # return (train_images, train_labels, train_paths), (test_images, test_labels, test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8d02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs_with_prediction(image_paths, actual_labels, probabilities):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    num_images = 25\n",
    "    # image_slice = random.sample(image_paths, num_images)\n",
    "\n",
    "    for n in range(num_images):\n",
    "        ax = plt.subplot(5, 5, n + 1)\n",
    "        img = mpimg.imread(image_paths[n])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "        probability_vector = probabilities[n]\n",
    "        highest_index = np.argmax(probability_vector)\n",
    "        print(f\"Probability Vector: {probability_vector}, highest index: {highest_index}\")\n",
    "\n",
    "        actual_label_vector = actual_labels[n]\n",
    "        correct_label = None\n",
    "        for label in DifficultyLevels.values():\n",
    "            label_vector = DifficultyLevels.get_one_hot_encoding(label)\n",
    "            if all(label_vector == actual_label_vector):\n",
    "                correct_label = label\n",
    "                break\n",
    "\n",
    "        for label in DifficultyLevels.values():\n",
    "            label_vector = DifficultyLevels.get_one_hot_encoding(label)\n",
    "            if label_vector[highest_index]:\n",
    "                plt.title(f\"{probability_vector[highest_index] * 100:.0f}% {label} ({correct_label})\")\n",
    "                break\n",
    "\n",
    "    plt.savefig(os.path.join(results_folder, 'result_classification.png'))\n",
    "\n",
    "\n",
    "def test_model(test_data: tuple, model_path, checkpoint_folder_name):\n",
    "    images_test, labels_test, paths_test = test_data\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        loaded_model = keras.models.load_model(model_path)\n",
    "        print(\"Model successfully loaded\")\n",
    "\n",
    "        prediction = loaded_model.predict(images_test)\n",
    "        # print(f\"Prediction result: {prediction}\")\n",
    "        show_imgs_with_prediction(paths_test, labels_test, prediction)\n",
    "\n",
    "        test_loss, test_acc = loaded_model.evaluate(images_test, labels_test, verbose=1)\n",
    "        print(\"Test accuracy: \", test_acc * 100)\n",
    "\n",
    "        # load latest (i.e. the best) checkpoint\n",
    "        \"\"\"\n",
    "        loaded_model = keras.models.load_model(model_path)  # re-create the model first!\n",
    "        checkpoint_folder = os.path.join(results_folder, checkpoint_folder_name)\n",
    "        latest = tf.train.latest_checkpoint(checkpoint_folder)\n",
    "        loaded_model.load_weights(latest)\n",
    "\n",
    "        # and re-evaluate the model\n",
    "        loss, acc = loaded_model.evaluate(images_test, labels_test, verbose=1)\n",
    "        print(f\"Accuracy with restored model weights: {100 * acc:5.2f}%\")\n",
    "        \"\"\"\n",
    "    else:\n",
    "        sys.stderr.write(\"No saved model found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d0fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_sequential(input_shape, num_classes=NUMBER_OF_CLASSES):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            keras.layers.Dropout(0.25),\n",
    "\n",
    "            keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            keras.layers.Dropout(0.25),\n",
    "\n",
    "            keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            keras.layers.Dropout(0.25),\n",
    "\n",
    "            keras.layers.Flatten(),\n",
    "            # units in the last layer should be a power of two (e.g. 64, 128, 512, 1024)\n",
    "            keras.layers.Dense(units=1024, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.5),\n",
    "\n",
    "            # units=3 as we have 3 classes -> we need a vector that looks like this: [0.2, 0.5, 0.3]\n",
    "            keras.layers.Dense(units=num_classes, activation=\"softmax\")  # softmax for multi-class classification, see\n",
    "            # https://medium.com/deep-learning-with-keras/how-to-solve-classification-problems-in-deep-learning-with-tensorflow-keras-6e39c5b09501\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    # optimizer = SGD(lr=0.01, momentum=0.9)\n",
    "    # other optimizers like \"rmsprop\" or \"adamax\" ?\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9ed9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data: tuple, test_data: tuple, model_path, checkpoint_path):\n",
    "    images_train, labels_train, paths_train = train_data\n",
    "    images_test, labels_test, paths_test = test_data\n",
    "\n",
    "    # print(tf.config.list_physical_devices('GPU'))\n",
    "    # print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "    print(\"Len train data:\", len(images_train))\n",
    "    print(\"Len test data:\", len(images_test))\n",
    "\n",
    "    set_random_seed()\n",
    "\n",
    "    # image_shape = images_train.shape[1:]\n",
    "    image_shape = (NEW_IMAGE_SIZE, NEW_IMAGE_SIZE, 1)\n",
    "    model = build_model_sequential(input_shape=image_shape)\n",
    "\n",
    "    EPOCHS = 32\n",
    "    BATCH_SIZE = 32\n",
    "    # VALIDATION_SPLIT = 0.25\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(checkpoint_path, monitor='val_categorical_accuracy', verbose=1, mode=\"max\",\n",
    "                                          save_best_only=True, save_weights_only=True)\n",
    "    lr_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5,\n",
    "                                                    verbose=1)\n",
    "\n",
    "    # history = model.fit(images_train, labels_train, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,\n",
    "    #                     verbose=1, epochs=EPOCHS, callbacks=[checkpoint_callback, lr_callback])\n",
    "\n",
    "    history = model.fit(images_train, labels_train, batch_size=BATCH_SIZE, verbose=1, epochs=EPOCHS,\n",
    "                        validation_data=(images_test, labels_test), callbacks=[checkpoint_callback, lr_callback])\n",
    "    print(history.history)\n",
    "    model.save(model_path)\n",
    "\n",
    "    show_result_plot(history, EPOCHS, metric=\"categorical_accuracy\", output_folder=results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03444786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train=True, test=False):\n",
    "    # TODO don't use pickle\n",
    "    images_train = np.load(os.path.join(results_folder, 'train_images.npy'), allow_pickle=False)\n",
    "    labels_train = np.load(os.path.join(results_folder, 'train_labels.npy'), allow_pickle=False)\n",
    "    paths_train = np.load(os.path.join(results_folder, 'train_paths.npy'), allow_pickle=False)\n",
    "\n",
    "    images_test = np.load(os.path.join(results_folder, 'test_images.npy'), allow_pickle=False)\n",
    "    labels_test = np.load(os.path.join(results_folder, 'test_labels.npy'), allow_pickle=False)\n",
    "    paths_test = np.load(os.path.join(results_folder, 'test_paths.npy'), allow_pickle=False)\n",
    "\n",
    "    MODEL_NAME = 'Cognitive-Load-CNN-Model.h5'\n",
    "    model_save_location = os.path.join(results_folder, MODEL_NAME)\n",
    "\n",
    "    checkpoint_folder = \"checkpoints\"\n",
    "    checkpoint_path = os.path.join(results_folder, checkpoint_folder,\n",
    "                                   \"checkpoint-improvement-{epoch:02d}-{val_categorical_accuracy:.3f}.ckpt\")\n",
    "\n",
    "    if train:\n",
    "        train_model((images_train, labels_train, paths_train), (images_test, labels_test, paths_test),\n",
    "                    model_save_location, checkpoint_path)\n",
    "\n",
    "    if test:\n",
    "        # TODO should not be the ones used for validation in train_model() !\n",
    "        test_model((images_test, labels_test, paths_test), model_save_location, checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdba8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f96d35d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train data: 53056\n",
      "Len test data: 20222\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 126, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 61, 61, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 25,786,883\n",
      "Trainable params: 25,786,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/32\n",
      "1658/1658 [==============================] - 1917s 1s/step - loss: 1.0108 - categorical_accuracy: 0.4290 - val_loss: 1.9973 - val_categorical_accuracy: 0.3376\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.33760, saving model to ml_results\\checkpoints\\checkpoint-improvement-01-0.338.ckpt\n",
      "Epoch 2/32\n",
      "1658/1658 [==============================] - 2080s 1s/step - loss: 0.4378 - categorical_accuracy: 0.8161 - val_loss: 6.6554 - val_categorical_accuracy: 0.3394\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.33760 to 0.33943, saving model to ml_results\\checkpoints\\checkpoint-improvement-02-0.339.ckpt\n",
      "Epoch 3/32\n",
      "1658/1658 [==============================] - 1810s 1s/step - loss: 0.2539 - categorical_accuracy: 0.9010 - val_loss: 9.2843 - val_categorical_accuracy: 0.3338\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.33943\n",
      "Epoch 4/32\n",
      "1658/1658 [==============================] - 1840s 1s/step - loss: 0.1825 - categorical_accuracy: 0.9321 - val_loss: 10.4280 - val_categorical_accuracy: 0.3371\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.33943\n",
      "Epoch 5/32\n",
      "1658/1658 [==============================] - 1805s 1s/step - loss: 0.1480 - categorical_accuracy: 0.9454 - val_loss: 12.3082 - val_categorical_accuracy: 0.3373\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.33943\n",
      "Epoch 6/32\n",
      "1658/1658 [==============================] - 1739s 1s/step - loss: 0.1324 - categorical_accuracy: 0.9514 - val_loss: 10.7785 - val_categorical_accuracy: 0.3320\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.33943\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/32\n",
      "1344/1658 [=======================>......] - ETA: 5:13 - loss: 0.0820 - categorical_accuracy: 0.9706"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8304/2920870769.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train a machine learning model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8304/3963593806.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         train_model((images_train, labels_train, paths_train), (images_test, labels_test, paths_test),\n\u001b[0m\u001b[0;32m     20\u001b[0m                     model_save_location, checkpoint_path)\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8304/1974747515.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train_data, test_data, model_path, checkpoint_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m#                     verbose=1, epochs=EPOCHS, callbacks=[checkpoint_callback, lr_callback])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     history = model.fit(images_train, labels_train, batch_size=BATCH_SIZE, verbose=1, epochs=EPOCHS,\n\u001b[0m\u001b[0;32m     30\u001b[0m                         validation_data=(images_test, labels_test), callbacks=[checkpoint_callback, lr_callback])\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \"\"\"\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m       \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train a machine learning model\n",
    "main(train=True, test=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
